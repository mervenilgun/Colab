from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from dotenv import load_dotenv
import os

# Ortam deÄŸiÅŸkenlerini yÃ¼kle (.env dosyasÄ±ndan API key alÄ±nÄ±r)
load_dotenv()

# GPT-4 veya GPT-3.5 tanÄ±mÄ±
llm = ChatOpenAI(model="gpt-4", temperature=0.7)

# HafÄ±za sistemi
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

print("\nğŸ§  Asistan baÅŸlatÄ±ldÄ±. Ã‡Ä±kmak iÃ§in 'Ã§Ä±k' yazÄ±n.\n")

while True:
    user_input = input("ğŸ‘¤ Soru: ")
    if user_input.lower() in ["Ã§Ä±k", "exit", "quit"]:
        print("ğŸ›‘ Asistan kapatÄ±ldÄ±.")
        break

    # GeÃ§miÅŸi al
    history = memory.load_memory_variables({})["chat_history"]
    formatted_history = "\n".join([f"{m.type.capitalize()}: {m.content}" for m in history])

    # Model iÃ§in Ã¶zel prompt
    full_prompt = (
        f"AÅŸaÄŸÄ±da bir kullanÄ±cÄ±yla Ã¶nceki konuÅŸmalar yer alÄ±yor.\n"
        f"{formatted_history}\n\n"
        f"KullanÄ±cÄ±nÄ±n yeni sorusu: {user_input}\n"
        f"GeÃ§miÅŸ konuÅŸmalara uygun ve baÄŸlamlÄ± TÃ¼rkÃ§e bir cevap ver."
    )

    # LLM yanÄ±tÄ±
    try:
        response = llm.invoke(full_prompt)
        print("ğŸ¤– Cevap:", response.content, "\n")
        # HafÄ±zaya ekle
        memory.chat_memory.add_user_message(user_input)
        memory.chat_memory.add_ai_message(response.content)
    except Exception as e:
        print("âŒ Hata:", e)
